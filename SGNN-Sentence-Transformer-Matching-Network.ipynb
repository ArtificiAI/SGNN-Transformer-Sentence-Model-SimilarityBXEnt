{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "\n",
    "This notebook contains code from: https://github.com/harvardnlp/annotated-transformer\n",
    "\n",
    "Original license: \n",
    "\n",
    "> MIT License\n",
    "> \n",
    "> Copyright (c) 2018 Alexander Rush\n",
    "> \n",
    "> Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "> of this software and associated documentation files (the \"Software\"), to deal\n",
    "> in the Software without restriction, including without limitation the rights\n",
    "> to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "> copies of the Software, and to permit persons to whom the Software is\n",
    "> furnished to do so, subject to the following conditions:\n",
    "> \n",
    "> The above copyright notice and this permission notice shall be included in all\n",
    "> copies or substantial portions of the Software.\n",
    "> \n",
    "> THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "> IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "> FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "> AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "> LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "> OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "> SOFTWARE.\n",
    "\n",
    "\n",
    "## License\n",
    "\n",
    "The new work is licensed under the BSD 3-Clause License.\n",
    "\n",
    "\n",
    "Copyright (c) 2018, Guillaume Chevalier\n",
    "\n",
    "All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.read_txt import *\n",
    "from src.data.config import *\n",
    "from src.data.training_data import *\n",
    "from src.data.sgnn_projection_layer import *\n",
    "from src.model.loss import *\n",
    "from src.model.transformer import *\n",
    "\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model_trainer):\n",
    "    d_model = model_trainer.sentence_projection_model.encoder.layers[0].size\n",
    "    return NoamOpt(d_model, 2, 4000,\n",
    "            torch.optim.Adam(model_trainer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "def run_epoch(epoch, model_trainer, model_opt, data_batch_iterator, cuda_device_id):\n",
    "    \"\"\"\n",
    "    Standard Training and Logging Function.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    mod_tokens = 0\n",
    "    mod = 10\n",
    "    \n",
    "    for i, (src, mask, category_per_sentence) in enumerate(data_batch_iterator):\n",
    "        target_diagonal_block_matrix = categories_to_block_matrix(category_per_sentence)\n",
    "        if cuda_device_id is not None:\n",
    "            src = src.cuda(cuda_device_id)\n",
    "            mask = mask.cuda(cuda_device_id)\n",
    "            target_diagonal_block_matrix = target_diagonal_block_matrix.cuda(cuda_device_id)\n",
    "                \n",
    "        # forward.\n",
    "        loss = model_trainer(src, mask, target_diagonal_block_matrix)\n",
    "        total_loss += loss\n",
    "        ntokens = (mask != 0.0).data.sum().item()\n",
    "        total_tokens += ntokens\n",
    "        mod_tokens += ntokens\n",
    "        \n",
    "        # backward.\n",
    "        if model_opt is not None:\n",
    "            loss.backward()\n",
    "            model_opt.step()\n",
    "            model_opt.optimizer.zero_grad()\n",
    "        \n",
    "        # log.\n",
    "        if i % mod == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch %d Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (epoch, i, loss / ntokens, mod_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            mod_tokens = 0\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "MY_MODEL_NAME = \"my-model{}\"\n",
    "\n",
    "def save_model(preproc_sgnn_sklearn_pipeline, sentence_projection_model, model_name=MY_MODEL_NAME):\n",
    "    a = model_name.format(\".sklearn\")\n",
    "    b = model_name.format(\".pytorch\")\n",
    "    dump(preproc_sgnn_sklearn_pipeline, a)\n",
    "    with open(b, \"wb\") as f:\n",
    "        torch.save(sentence_projection_model, f=f)\n",
    "    print(\"Saved model to files:\", a, b)\n",
    "\n",
    "\n",
    "def load_model(model_name=MY_MODEL_NAME):\n",
    "    a = model_name.format(\".sklearn\")\n",
    "    b = model_name.format(\".pytorch\")\n",
    "    preproc_sgnn_sklearn_pipeline = load(a)\n",
    "    sentence_projection_model = torch.load(b)\n",
    "    print(\"Loaded model from files:\", a, b)\n",
    "    return preproc_sgnn_sklearn_pipeline, sentence_projection_model\n",
    "\n",
    "# preproc_sgnn_sklearn_pipeline = get_sgnn_projection_pipeline()\n",
    "# sentence_projection_model = make_sentence_model()\n",
    "# save_model(preproc_sgnn_sklearn_pipeline, sentence_projection_model)\n",
    "# preproc_sgnn_sklearn_pipeline, sentence_projection_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !du -sh my-model*\n",
    "import glob  # todo\n",
    "\n",
    "def load_most_recent_model(model_name):\n",
    "    a = model_name.format(\".sklearn*\")\n",
    "    b = model_name.format(\".pytorch*\")\n",
    "    a = list(sorted(glob.glob(a)))[-1]  # model with highest epoch number\n",
    "    b = list(sorted(glob.glob(b)))[-1]  # model with highest epoch number\n",
    "    \n",
    "    suffix = a.split(model_name.format(\".sklearn\"))[-1]\n",
    "    return load_model(model_name + suffix)\n",
    "\n",
    "# preproc_sgnn_sklearn_pipeline, sentence_projection_model = load_most_recent_model(MY_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from files: my-model.sklearn.epoch_9 my-model.pytorch.epoch_9\n",
      "Epoch 0 Step: 0 Loss: 0.013953 Tokens per Sec: 84.084095\n",
      "Epoch 0 Step: 10 Loss: 0.004901 Tokens per Sec: 161.256825\n",
      "Epoch 0 Step: 20 Loss: 0.003658 Tokens per Sec: 108.806568\n",
      "Epoch 0 Step: 30 Loss: 0.002423 Tokens per Sec: 96.876060\n",
      "Epoch 0 Step: 0 Loss: 0.009643 Tokens per Sec: 96.005717\n",
      "Saved model to files: my-model.sklearn.epoch_00000 my-model.pytorch.epoch_00000\n",
      "Epoch 1 Step: 0 Loss: 0.014455 Tokens per Sec: 103.015428\n",
      "Epoch 1 Step: 10 Loss: 0.004191 Tokens per Sec: 140.481162\n",
      "Epoch 1 Step: 20 Loss: 0.003677 Tokens per Sec: 90.253537\n",
      "Epoch 1 Step: 30 Loss: 0.002752 Tokens per Sec: 89.502752\n",
      "Epoch 1 Step: 0 Loss: 0.009006 Tokens per Sec: 71.826464\n",
      "Saved model to files: my-model.sklearn.epoch_00001 my-model.pytorch.epoch_00001\n",
      "Epoch 2 Step: 0 Loss: 0.011542 Tokens per Sec: 76.331674\n",
      "Epoch 2 Step: 10 Loss: 0.002441 Tokens per Sec: 110.938335\n",
      "Epoch 2 Step: 20 Loss: 0.003203 Tokens per Sec: 100.831212\n",
      "Epoch 2 Step: 30 Loss: 0.001809 Tokens per Sec: 99.494122\n",
      "Epoch 2 Step: 0 Loss: 0.006356 Tokens per Sec: 120.978972\n",
      "Saved model to files: my-model.sklearn.epoch_00002 my-model.pytorch.epoch_00002\n",
      "Epoch 3 Step: 0 Loss: 0.011179 Tokens per Sec: 108.424544\n",
      "Epoch 3 Step: 10 Loss: 0.002443 Tokens per Sec: 153.824657\n",
      "Epoch 3 Step: 20 Loss: 0.005057 Tokens per Sec: 112.665054\n",
      "Epoch 3 Step: 30 Loss: 0.002535 Tokens per Sec: 133.013183\n",
      "Epoch 3 Step: 0 Loss: 0.007189 Tokens per Sec: 104.677322\n",
      "Saved model to files: my-model.sklearn.epoch_00003 my-model.pytorch.epoch_00003\n",
      "Epoch 4 Step: 0 Loss: 0.007673 Tokens per Sec: 113.826060\n",
      "Epoch 4 Step: 10 Loss: 0.002980 Tokens per Sec: 121.222250\n",
      "Epoch 4 Step: 20 Loss: 0.002217 Tokens per Sec: 90.567674\n",
      "Epoch 4 Step: 30 Loss: 0.002535 Tokens per Sec: 100.673172\n"
     ]
    }
   ],
   "source": [
    "%%snakeviz\n",
    "\n",
    "batch_size = 25\n",
    "train_iters_per_epoch = 40\n",
    "test_iters_per_epoch = 1\n",
    "max_epoch = 10\n",
    "\n",
    "# CUDA\n",
    "cuda_device_id = None  # None for CPU, 0 for first GPU, etc.\n",
    "if cuda_device_id is not None:\n",
    "    context = torch.cuda.device(device_id)\n",
    "    context.__enter__()\n",
    "\n",
    "# Create model\n",
    "# todo: load or not? bool param.\n",
    "# preproc_sgnn_sklearn_pipeline = get_sgnn_projection_pipeline()\n",
    "# sentence_projection_model = make_sentence_model(d_ff=1024)\n",
    "preproc_sgnn_sklearn_pipeline, sentence_projection_model = load_most_recent_model(MY_MODEL_NAME)\n",
    "model_trainer = TrainerModel(sentence_projection_model)\n",
    "model_opt = get_std_opt(model_trainer)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    model_trainer.train()\n",
    "    run_epoch(\n",
    "        epoch, model_trainer, model_opt,\n",
    "        DataBatchIterator(preproc_sgnn_sklearn_pipeline, max_iters=train_iters_per_epoch),\n",
    "        cuda_device_id\n",
    "    )\n",
    "\n",
    "    model_trainer.eval()\n",
    "    run_epoch(\n",
    "        epoch, model_trainer, model_opt,\n",
    "        DataBatchIterator(preproc_sgnn_sklearn_pipeline, max_iters=test_iters_per_epoch),\n",
    "        cuda_device_id\n",
    "    )\n",
    "    epoch_model_name = MY_MODEL_NAME + \".epoch_\" + str(epoch).rjust(5, \"0\")\n",
    "    save_model(preproc_sgnn_sklearn_pipeline, sentence_projection_model, epoch_model_name)\n",
    "\n",
    "# CUDA\n",
    "if cuda_device_id is not None:\n",
    "    context.__exit__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size = 36000000\n",
    "batch_size = 25\n",
    "max_epoch = 24\n",
    "train_iters_per_epoch = 2400\n",
    "test_iters_per_epoch = 1\n",
    "# print(dataset_size/25)  # 1 440 000\n",
    "# 100000 steps  = 100000*25 sentences = 2 500 000\n",
    "\n",
    "# so I should have: epoch * steps_per_epochs = 57600\n",
    "\n",
    "# batch_size = 25 sentences\n",
    "# train_iters_per_epoch = 40\n",
    "# test_iters_per_epoch = 1\n",
    "# max_epoch = 10\n",
    "\n",
    "# English-French dataset consisting of 36M (36000000) sentences\n",
    "# Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens\n",
    "# Trained 100,000 steps or 12 hours\n",
    "# each training step took about 0.4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data_gen(1000, 32, 777).__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.src_mask.shape, c.src.shape\n",
    "# (torch.Size([32, 1, 10]), torch.Size([32, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTF8_TXT_RAW_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# preproc_sgnn_sklearn_pipeline, sentence_projection_model#  = load_most_recent_model(MY_MODEL_NAME)\n",
    "# model_trainer = TrainerModel(sentence_projection_model)\n",
    "\n",
    "sentence_projection = preproc_sgnn_sklearn_pipeline.transform((\n",
    "    \"This is a test. This is another test. \"\n",
    "    \"I like bacon. I don't like bacon. \"\n",
    "    \"My name is Guillaume. My family name is Chevalier. \"\n",
    "    \"Programming can be used for solving complicated math problems. Let's use the Python language to write some scientific code. \"\n",
    "    \"My family regrouped for Christmast. We met aunts and uncles. \"\n",
    "    \"I like linux. I have an operating system. \"\n",
    "    \"Have you ever been in the situation where you've got Jupyter notebooks (iPython notebooks) so huge that you were feeling stuck in your code?. Or even worse: have you ever found yourself duplicating your notebook to do changes, and then ending up with lots of badly named notebooks?. Either and in any ways. For every medium to big application. \"\n",
    "    \"If you're working with notebooks, it is highly likely that you're doing research and development. If doing research and development, to keep your amazing-10x-working-speed-multiplier, it might be a good idea to skip unit tests. I hope you were satisfied by this reading. What would you do?.\"\n",
    ").split(\". \"))\n",
    "category_per_sentence = [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9]\n",
    "projected_words, mask = pad_right(sentence_projection)\n",
    "\n",
    "\n",
    "sentence_projection = sentence_projection_model(projected_words, mask)\n",
    "prediction = matching_network_self_attention(sentence_projection)\n",
    "clipped_prediction = ((prediction - prediction.mean() - prediction.std()) > 0)\n",
    "target_diagonal_block_matrix = categories_to_block_matrix(category_per_sentence)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(prediction.data.numpy())\n",
    "plt.show()\n",
    "plt.imshow(clipped_prediction.data.numpy())\n",
    "plt.show()\n",
    "plt.imshow(target_diagonal_block_matrix)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
